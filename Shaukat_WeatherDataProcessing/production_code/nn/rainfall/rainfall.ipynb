{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input : BOM HISTORICAL file\n",
    "\n",
    "# Output: given BOM and GFS features for current time, produces rainfall forecast for the CURRENT hour, although groundtruth is \n",
    "# available for current time. The logic is this, we will forecast rainfall on the basis of future values of GFS and BOM\n",
    "# at timesteps 't+1, t+2 ... t+240' to produce rainfall at timesteps 't+1, t+2 ... t+240'\n",
    "\n",
    "# Testing: GFS+BOM inputs will be used cross validation. During real-time testing, \n",
    "# BOM's variables will be injected from our own predictions \n",
    "\n",
    "# NOTES: If the groundtruth dataframe contains NaNs due to missing values in BOM Historical file, then these NaN entries will\n",
    "#        be replaced by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "# Import modules\n",
    "#####################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import scipy.stats as stats\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from copy import deepcopy\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "################################\n",
    "# To generate plots in notebook\n",
    "###############################\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Setting Training ratio\n",
    "########################\n",
    "train_ratio = 0.8\n",
    "\n",
    "#############################\n",
    "# Loading BOM and GFS files\n",
    "#############################\n",
    "df_gfs = pd.read_csv(\"C:/Shaukat/code/data_rep/gfs/2016_2017/csv/merimbula_1hr.csv\")\n",
    "df_bom = pd.read_csv(\"C:/Shaukat/code/data_rep/bom/historical_data/2013-2016/NSW/csv/HM01X_Data_069147_999999999405558_merimbula_airport.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# Check and fill null values of dataframe column by value provided \n",
    "###################################################################\n",
    "\n",
    "def check_and_fill_null_entries_df_col_by_value(df, col_name, val=0.0):\n",
    "    # Check if dataframe is null\n",
    "    is_df_nan = df[col_name].isnull().any()\n",
    "    if (is_df_nan):\n",
    "        print 'col %s contains null values. Filling these null values by %f' %(col_name, val)\n",
    "        df[col_name].fillna(val, inplace=True)\n",
    "\n",
    "    # Check again if dataframe column is null\n",
    "    is_df_nan = df[col_name].isnull().any()\n",
    "    if not is_df_nan:\n",
    "        print '%s doesnot contain null values. PROCEED' %(col_name)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# Check and backfill null values of dataframe column \n",
    "################################################\n",
    "\n",
    "def check_and_fill_null_entries_df_col_backwards(df, col_name):\n",
    "    # Check if dataframe is null\n",
    "    is_df_nan = df[col_name].isnull().any()\n",
    "    if (is_df_nan):\n",
    "        print 'col %s contains null values. Filling these null values' %(col_name)\n",
    "        df[col_name].fillna(method='bfill', inplace=True)\n",
    "\n",
    "    # Check again if dataframe column is null\n",
    "    is_df_nan = df[col_name].isnull().any()\n",
    "    if not is_df_nan:\n",
    "        print '%s doesnot contain null values. PROCEED' %(col_name)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# Check and fill null values of dataframe \n",
    "################################################\n",
    "\n",
    "def check_and_fill_null_entries_df(df):\n",
    "    # Check if dataframe is null\n",
    "    is_df_nan = df.isnull().any().any()\n",
    "    if (is_df_nan):\n",
    "        print 'df contains some null values. Filling these null values'\n",
    "        df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "    # Check again if dataframe is null\n",
    "    is_df_nan = df.isnull().any().any()\n",
    "    if not is_df_nan:\n",
    "        print 'passed df doesnot contain null values. PROCEED'\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# Check and ffill null values of dataframe column \n",
    "################################################\n",
    "\n",
    "def check_and_fill_null_entries_df_col(df, col_name):\n",
    "    # Check if dataframe is null\n",
    "    is_df_nan = df[col_name].isnull().any()\n",
    "    if (is_df_nan):\n",
    "        print 'col %s contains null values. Filling these null values' %(col_name)\n",
    "        df[col_name].fillna(method='ffill', inplace=True)\n",
    "\n",
    "    # Check again if dataframe column is null\n",
    "    is_df_nan = df[col_name].isnull().any()\n",
    "    if not is_df_nan:\n",
    "        print '%s doesnot contain null values. PROCEED' %(col_name)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# Setting up GFS\n",
    "####################\n",
    "df_gfs['Unnamed: 0'] = pd.to_datetime(df_gfs['Unnamed: 0'])\n",
    "df_gfs.set_index('Unnamed: 0', inplace = True)\n",
    "\n",
    "########################\n",
    "# Setting up BOM\n",
    "#######################\n",
    "df_bom.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df_bom.utc_mmddyyyy = pd.to_datetime(df_bom.utc_mmddyyyy)\n",
    "df_bom.set_index('utc_mmddyyyy', inplace=True)\n",
    "\n",
    "####################################\n",
    "# Remove duplicated indices in bom\n",
    "####################################\n",
    "df_bom = df_bom.groupby(df_bom.index).first() # Remove the duplicated index\n",
    "\n",
    "#####################################################\n",
    "# Get the mutual start and end dates from BOM and GFS\n",
    "#####################################################\n",
    "if df_gfs.index[0]>df_bom.index[0]:\n",
    "    start_date = df_gfs.index[0]\n",
    "else:\n",
    "    start_date = df_bom.index[0]\n",
    "\n",
    "if df_gfs.index[len(df_gfs)-1]<df_bom.index[len(df_bom)-1]:\n",
    "    end_date = df_gfs.index[len(df_gfs)-1]\n",
    "else:\n",
    "    end_date = df_bom.index[len(df_bom)-1]\n",
    "    \n",
    "print start_date, end_date\n",
    "\n",
    "##############################################\n",
    "# Chop GFS, BOM and GT from start to end date\n",
    "#############################################\n",
    "df_bom = df_bom.loc[start_date:end_date]\n",
    "df_gfs = df_gfs.loc[start_date:end_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(df_bom)\n",
    "print len(df_gfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Merge GFS and BOM using intersection, into one dataframe\n",
    "###########################################################\n",
    "df_gfs_bom_combined = df_bom.merge(df_gfs, how='inner', left_index=True, right_index=True)\n",
    "print len(df_gfs_bom_combined)\n",
    "df_gfs_bom_combined.dropna(axis=0, how='all', inplace = True) # Drop the rows where all of the elements are nan\n",
    "print len(df_gfs_bom_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############################################\n",
    "# Processing BOM's selected columns to numeric\n",
    "###############################################\n",
    "\n",
    "df_gfs_bom_combined['Air Temperature in degrees C'] = pd.to_numeric(df_gfs_bom_combined['Air Temperature in degrees C'], errors='coerce')\n",
    "df_gfs_bom_combined['Station level pressure in hPa'] = pd.to_numeric(df_gfs_bom_combined['Station level pressure in hPa'], errors='coerce')\n",
    "df_gfs_bom_combined['Relative humidity in percentage %'] = pd.to_numeric(df_gfs_bom_combined['Relative humidity in percentage %'], errors='coerce')\n",
    "df_gfs_bom_combined['rainfall_since_last_hour'] = pd.to_numeric(df_gfs_bom_combined['rainfall_since_last_hour'], errors='coerce')\n",
    "\n",
    "##########################################\n",
    "# Columns to check for NaN\n",
    "###########################################\n",
    "col_list_bom = ['Air Temperature in degrees C', 'Station level pressure in hPa','Relative humidity in percentage %']\n",
    "\n",
    "for _col in col_list_bom:\n",
    "    df_gfs_bom_combined = check_and_fill_null_entries_df_col(df_gfs_bom_combined,_col)\n",
    "\n",
    "del col_list_bom\n",
    "\n",
    "#################################\n",
    "# Replace NaN in rainfall by 0.0\n",
    "#################################\n",
    "df_gfs_bom_combined = check_and_fill_null_entries_df_col_by_value(df_gfs_bom_combined, 'rainfall_since_last_hour', 0.0)\n",
    "\n",
    "########################\n",
    "# Prepare GT dataframe\n",
    "########################\n",
    "df_gt = pd.DataFrame(index=df_gfs_bom_combined.index)\n",
    "df_gt['rainfall_since_last_hour'] = df_gfs_bom_combined['rainfall_since_last_hour'].copy()\n",
    "df_gfs_bom_combined.drop('rainfall_since_last_hour', axis=1, inplace=True) # Drop the groundtruth from main dataframe\n",
    "\n",
    "print len(df_gt)\n",
    "print len(df_gfs_bom_combined) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# Convert temperature and pressure to the units consistent with BOM\n",
    "# i.e. Kelvin and Hpa\n",
    "###################################################################\n",
    "\n",
    "df_gfs_bom_combined['pred_temp'] = df_gfs_bom_combined['pred_temp'] - 273.15 # Kelvin to Celsius (BOM unit)\n",
    "df_gfs_bom_combined['pred_surface_pressure'] = df_gfs_bom_combined['pred_surface_pressure']/100.0 # Pa to Hpa (BOM Unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# fill the null entries of combined dataframe\n",
    "###########################################################\n",
    "col_list = ['Air Temperature in degrees C', 'Relative humidity in percentage %','Station level pressure in hPa',\n",
    "           'pred_cloud_cover', 'pred_cloud_cover_bound_cloud_layer', 'pred_convective_cloud',\n",
    "           'pred_dewp', 'pred_high_tcc', 'pred_low_tcc', 'pred_lw_rad', 'pred_max_wind_press', 'pred_merid_wind',\n",
    "           'pred_middle_tcc', 'pred_rain_rate', 'pred_rel_humidity', 'pred_sunshine', 'pred_surf_geowind',\n",
    "           'pred_surface_pressure', 'pred_sw_rad', 'pred_temp', 'pred_total_rain', 'pred_ustorm', \n",
    "            'pred_vstorm', 'pred_wind_speed_surf', 'pred_zonal_wind']\n",
    "\n",
    "for _col in col_list:\n",
    "    df_gfs_bom_combined = check_and_fill_null_entries_df_col(df_gfs_bom_combined,_col)\n",
    "\n",
    "########################################################################\n",
    "# get the datframe for data and prediction as df_X and df_Y respectively\n",
    "########################################################################\n",
    "df_X = df_gfs_bom_combined[col_list].copy()\n",
    "df_Y = df_gt.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Make sure that df_X is not null\n",
    "######################################\n",
    "df_X_bool = df_X.isnull().any().any()\n",
    "print df_X_bool, 'THIS MUST BE FALSE' # This must be FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare test observations from GFS\n",
    "# col_list_gfs = ['pred_temp', 'pred_surface_pressure', 'pred_rel_humidity',\n",
    "#            'temp_p1', 'press_p1', 'hum_p1']\n",
    "# df_X_test = df_gfs[col_list_gfs].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make sure that df_X is not null\n",
    "# df_X_bool = df_X_test.isnull().any().any()\n",
    "# print df_X_bool, 'THIS MUST BE FALSE' # This must be FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# See if the pressure from BOM and GFS have linear relation among them\n",
    "# plt.scatter(df_X.Pressure.values,df_X.pred_surface_pressure.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.scatter(df_X['Air Temperature in degrees C'].values,df_X.pred_temp.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_X.Pressure.mean()\n",
    "# print df_X['Mean sea level pressure in hPa'].min(), df_X['Mean sea level pressure in hPa'].max() \n",
    "# print df_X['Station level pressure in hPa'].min(), df_X['Station level pressure in hPa'].max()\n",
    "\n",
    "###########################################################\n",
    "# Check the pressure values and probably flag the outliers\n",
    "###########################################################\n",
    "\n",
    "print 'min_pressure_bomstation: %f max_pressure_bomstation=%f\\n'%(df_X['Station level pressure in hPa'].min(),df_X['Station level pressure in hPa'].max()) \n",
    "print 'min_pressure_gfs: %f max_pressure_gfs=%f\\n'%(df_X.pred_surface_pressure.min(), df_X.pred_surface_pressure.max()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.scatter(df_X['Station level pressure in hPa'].values,df_X.pred_surface_pressure.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_X['pred_surface_pressure'].loc[df_X['pred_surface_pressure'] > 1018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If pressure is not having a linear relation with GFS predicted pressure, then replace unaccepted values by mean\n",
    "# df_X['Station level pressure in hPa'].loc[df_X['Station level pressure in hPa'] < 900] = df_X.Pressure.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#############################\n",
    "# Scaling the training set\n",
    "#############################\n",
    "\n",
    "tot_points = len(df_X)\n",
    "train_points = int(np.floor(train_ratio*tot_points)) \n",
    "print 'tot_points: ', tot_points, ' train_points: ', train_points\n",
    "\n",
    "df_data_matrix = df_X.as_matrix()\n",
    "print 'Shape of data matrix: ',df_data_matrix.shape\n",
    "\n",
    "# Generate Train Sequence\n",
    "x_train = np.copy(df_data_matrix[0:train_points + 1,:])\n",
    "\n",
    "# Add polynomial features\n",
    "# poly = PolynomialFeatures(degree=2, interaction_only=False)\n",
    "# x_train = poly.fit_transform(x_train)\n",
    "\n",
    "# Normalize data matrix with zero mean and unit covariance\n",
    "# http://scikit-learn.org/stable/modules/preprocessing.html\n",
    "scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "\n",
    "del tot_points\n",
    "del train_points\n",
    "del df_data_matrix\n",
    "del x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################################\n",
    "# Prepare dataframes for FF Network\n",
    "####################################\n",
    "\n",
    "df_data_matrix = df_X.as_matrix()\n",
    "df_gt_rainfall_forecast_matrix = df_Y.as_matrix()\n",
    "\n",
    "# Adding Polynomial features\n",
    "# Apply poly transform on df_data_matrix\n",
    "# df_data_matrix = poly.fit_transform(df_data_matrix)\n",
    "\n",
    "print 'Shape of data matrix: ',df_data_matrix.shape\n",
    "print 'Shape of label matrix: ',df_gt_rainfall_forecast_matrix.shape\n",
    "\n",
    "# Normalize data matrix with zero mean and unit covariance\n",
    "# http://scikit-learn.org/stable/modules/preprocessing.html\n",
    "# df_data_matrix = preprocessing.scale(df_data_matrix)\n",
    "df_data_matrix = scaler.transform(df_data_matrix)\n",
    "\n",
    "\n",
    "data_matrix = np.copy(df_data_matrix)\n",
    "label_matrix = np.copy(df_gt_rainfall_forecast_matrix)\n",
    "\n",
    "print \"InputFeedForwardNetwork: \",data_matrix.shape\n",
    "print \"OutputFeedForwardNetwork: \",label_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Extract train and test data\n",
    "###########################################################\n",
    "\n",
    "# Prepare test and train points\n",
    "tot_points = data_matrix.shape[0]\n",
    "train_points = int(np.floor(train_ratio*tot_points)) \n",
    "test_points = tot_points - train_points\n",
    "print 'tot_points: ', tot_points, ' train_points: ', train_points, ' test_points: ', test_points\n",
    "\n",
    "# Generate Train Sequence\n",
    "x_train = np.copy(data_matrix[0:train_points + 1,:])\n",
    "y_train = np.copy(label_matrix[0:train_points + 1,:])\n",
    "# Generate Test Sequence\n",
    "x_test = np.copy(data_matrix[train_points + 1:tot_points + 1 ,:])\n",
    "y_test = np.copy(label_matrix[train_points + 1:tot_points + 1 ,:])\n",
    "\n",
    "print 'x_train_shape: ', x_train.shape, ' y_train_shape: ', y_train.shape\n",
    "print 'x_test_shape: ', x_test.shape, ' y_test_shape: ', y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'train rainfall in mm: %f' %(np.sum(y_train))\n",
    "print 'test rainfall in mm: %f' %(np.sum(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'start and end date of train: ',df_Y.index[0], df_Y.index[train_points-1]\n",
    "print 'start and end date of test: ',df_Y.index[train_points], df_Y.index[tot_points-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "# Create model with three hidden layers, tried several combinations and found this one very useful\n",
    "#####################################################################################################\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=x_train.shape[1], init='normal', activation='relu')) #1st Hidden Layer\n",
    "model.add(Dense(200, init='normal', activation='relu')) #2nd Hidden Layer\n",
    "model.add(Dense(300, init='normal', activation='relu')) #3rd Hidden Layer\n",
    "model.add(Dense(y_train.shape[1], init='normal')) #output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################\n",
    "# Register callbacks for model and csv_file\n",
    "###########################################\n",
    "\n",
    "\n",
    "# filepath_savemodel = 'C:/Shaukat/code/rainfall/model_stored/feedforward/cygnet/bomgfs_input_bom_output/weights.{epoch:02d}-{val_loss:.2f}.h5'\n",
    "filepath_savemodel = 'C:/Shaukat/code/rainfall/model_stored/feedforward/cygnet/bomgfs_input_bom_output/model_v1_best.h5'\n",
    "# filepath_csvlogger = 'C:/Shaukat/code/rainfall/model_stored/feedforward/cygnet/bomgfs_input_bom_output/training_s2.log'\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath_savemodel, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "# csv_logger = CSVLogger(filepath_csvlogger, separator=',', append=False)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0.001)\n",
    "\n",
    "# history = model.fit(x_train, y_train, nb_epoch=1000, verbose=1, validation_data=(x_test, y_test), batch_size = 32, shuffle=True, callbacks=[checkpointer, csv_logger, reduce_lr])\n",
    "history = model.fit(x_train, y_train, nb_epoch=100, verbose=1, validation_data=(x_test, y_test), batch_size = 32, shuffle=True, callbacks=[checkpointer, reduce_lr])\n",
    "# history = model.fit(x_train, y_train, nb_epoch=100, verbose=1, validation_data=(x_test, y_test), batch_size = train_points, shuffle=True, callbacks=[checkpointer])\n",
    "# history = model.fit(x_train, y_train, nb_epoch=500, verbose=1, validation_data=(x_test, y_test), batch_size = 32, callbacks=[checkpointer])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "# Save the model at last epoch\n",
    "###############################\n",
    "model.save('C:/Shaukat/code/rainfall/model_stored/feedforward/cygnet/bomgfs_input_bom_output/model_v1_last.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "# Prediction on Test Set\n",
    "###############################\n",
    "\n",
    "vec_test = np.copy(y_test)\n",
    "print vec_test.shape\n",
    "print 'total test points: ', vec_test.shape[0]\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "pred_vec = np.copy(predictions)\n",
    "print pred_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Replace the prediction<threshold by 0.0\n",
    "#########################################\n",
    "threshold = 0.01\n",
    "preproc_y = deepcopy(pred_vec)\n",
    "preproc_y[preproc_y<=threshold] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "# Plot groundtruth vs predictions\n",
    "##################################\n",
    "\n",
    "time_index = np.arange(0,vec_test.shape[0],1)\n",
    "time_index = time_index.reshape(((len(time_index),1)))\n",
    "\n",
    "plt_lower_limit = 0\n",
    "plt_upper_limit = 1000\n",
    "\n",
    "plt.figure(figsize=(25,8))\n",
    "\n",
    "###########\n",
    "# Full plot\n",
    "###########\n",
    "# plt.plot(time_index,vec_test,label=\"GroundTruth\",color='b')\n",
    "# plt.plot(time_index,pred_vec,label=\"Prediction\",color='g')\n",
    "\n",
    "######################\n",
    "# Unpreprocessed Plot\n",
    "#####################\n",
    "plt.plot(time_index[plt_lower_limit:plt_upper_limit],vec_test[plt_lower_limit:plt_upper_limit],label=\"GroundTruth\",color='b')\n",
    "plt.plot(time_index[plt_lower_limit:plt_upper_limit],pred_vec[plt_lower_limit:plt_upper_limit],label=\"Prediction\",color='g')\n",
    "\n",
    "####################\n",
    "# Preprocessed Plot\n",
    "###################\n",
    "# plt.plot(time_index[plt_lower_limit:plt_upper_limit],vec_test[plt_lower_limit:plt_upper_limit],label=\"GroundTruth\",color='b')\n",
    "# plt.plot(time_index[plt_lower_limit:plt_upper_limit],preproc_y[plt_lower_limit:plt_upper_limit],label=\"Prediction\",color='g')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Rainfall prediction in mm')\n",
    "plt.legend()\n",
    "plt.title('Groundtruth and Predicted Rainfall per hour')\n",
    "# img_filename = 'C:\\\\Users\\\\ShaukatAbidi\\\\Documents\\\\shaukat_python_progs\\\\learning\\\\time_series\\\\feed_forward_nn\\\\models\\\\s7_images\\\\'+str(test_ex)+'.png'\n",
    "# plt.savefig(img_filename, bbox_inches='tight')\n",
    "# plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving an array to file\n",
    "#np.savetxt('predictions.out', predictions, delimiter=',')   # predictions is an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "# Evaluate training\n",
    "###############################\n",
    "model.evaluate(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########################################\n",
    "# Plot model prediction over train set\n",
    "########################################\n",
    "vec_train = np.copy(y_train)\n",
    "print vec_train.shape\n",
    "print 'total train points: ', vec_train.shape[0]\n",
    "\n",
    "pred_on_gt = model.predict(x_train)\n",
    "pred_on_gt_vec = np.copy(pred_on_gt)\n",
    "print pred_on_gt_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########\n",
    "# For plot\n",
    "##########\n",
    "time_index = np.arange(0,vec_train.shape[0],1)\n",
    "time_index = time_index.reshape(((len(time_index),1)))\n",
    "\n",
    "plt_lower_limit = 0\n",
    "plt_upper_limit = 500\n",
    "\n",
    "plt.figure(figsize=(25,8))\n",
    "\n",
    "############\n",
    "# Full plot\n",
    "############\n",
    "# plt.plot(time_index,vec_train,label=\"GroundTruth\",color='b')\n",
    "# plt.plot(time_index,pred_on_gt_vec,label=\"Prediction\",color='g')\n",
    "\n",
    "#######################\n",
    "# Unpreprocessed Plot\n",
    "#######################\n",
    "plt.plot(time_index[plt_lower_limit:plt_upper_limit],vec_train[plt_lower_limit:plt_upper_limit],label=\"GroundTruth\",color='b')\n",
    "plt.plot(time_index[plt_lower_limit:plt_upper_limit],pred_on_gt_vec[plt_lower_limit:plt_upper_limit],label=\"Prediction\",color='g')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Rainfall prediction in mm')\n",
    "plt.legend()\n",
    "plt.title('Groundtruth and Predicted Rainfall per hour')\n",
    "# img_filename = 'C:\\\\Users\\\\ShaukatAbidi\\\\Documents\\\\shaukat_python_progs\\\\learning\\\\time_series\\\\feed_forward_nn\\\\models\\\\s7_images\\\\'+str(test_ex)+'.png'\n",
    "# plt.savefig(img_filename, bbox_inches='tight')\n",
    "# plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#############################\n",
    "# plot y_train as timeseries\n",
    "#############################\n",
    "plt.plot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#############################\n",
    "# plot y_test as timeseries\n",
    "#############################\n",
    "plt.plot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:sequential_regression]",
   "language": "python",
   "name": "conda-env-sequential_regression-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
