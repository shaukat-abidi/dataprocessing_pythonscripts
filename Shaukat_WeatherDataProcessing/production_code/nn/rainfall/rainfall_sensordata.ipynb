{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input : BOM HISTORICAL file\n",
    "\n",
    "# Output: given BOM and GFS features for current time, produces rainfall forecast for the CURRENT hour, although groundtruth is \n",
    "# available for current time. The logic is this, we will forecast rainfall on the basis of future values of GFS and BOM\n",
    "# at timesteps 't+1, t+2 ... t+240' to produce rainfall at timesteps 't+1, t+2 ... t+240'\n",
    "\n",
    "# Testing: GFS+BOM inputs will be used cross validation. During real-time testing, \n",
    "# BOM's variables will become available from our own predictions )\n",
    "\n",
    "# NOTES: If the groundtruth dataframe contains NaNs due to missing values in BOM Historical file, then these NaN entries will\n",
    "#        be replaced by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import scipy.stats as stats\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from copy import deepcopy\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "df_gfs = pd.read_csv(\"C:/Shaukat/code/data_rep/gfs/2016_2017/csv/pooley_1hr.csv\")\n",
    "df_bom = pd.read_csv(\"C:/Shaukat/code/data_rep/sensor_data/houston_rainfall.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_and_fill_null_entries_df_col_by_value(df, col_name, val=0.0):\n",
    "    # Check if dataframe is null\n",
    "    is_df_nan = df[col_name].isnull().any()\n",
    "    if (is_df_nan):\n",
    "        print 'col %s contains null values. Filling these null values by %f' %(col_name, val)\n",
    "        df[col_name].fillna(val, inplace=True)\n",
    "\n",
    "    # Check again if dataframe column is null\n",
    "    is_df_nan = df[col_name].isnull().any()\n",
    "    if not is_df_nan:\n",
    "        print '%s doesnot contain null values. PROCEED' %(col_name)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_and_fill_null_entries_df_col_backwards(df, col_name):\n",
    "    # Check if dataframe is null\n",
    "    is_df_nan = df[col_name].isnull().any()\n",
    "    if (is_df_nan):\n",
    "        print 'col %s contains null values. Filling these null values' %(col_name)\n",
    "        df[col_name].fillna(method='bfill', inplace=True)\n",
    "\n",
    "    # Check again if dataframe column is null\n",
    "    is_df_nan = df[col_name].isnull().any()\n",
    "    if not is_df_nan:\n",
    "        print '%s doesnot contain null values. PROCEED' %(col_name)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_and_fill_null_entries_df(df):\n",
    "    # Check if dataframe is null\n",
    "    is_df_nan = df.isnull().any().any()\n",
    "    if (is_df_nan):\n",
    "        print 'df contains some null values. Filling these null values'\n",
    "        df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "    # Check again if dataframe is null\n",
    "    is_df_nan = df.isnull().any().any()\n",
    "    if not is_df_nan:\n",
    "        print 'passed df doesnot contain null values. PROCEED'\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_and_fill_null_entries_df_col(df, col_name):\n",
    "    # Check if dataframe is null\n",
    "    is_df_nan = df[col_name].isnull().any()\n",
    "    if (is_df_nan):\n",
    "        print 'col %s contains null values. Filling these null values' %(col_name)\n",
    "        df[col_name].fillna(method='ffill', inplace=True)\n",
    "\n",
    "    # Check again if dataframe column is null\n",
    "    is_df_nan = df[col_name].isnull().any()\n",
    "    if not is_df_nan:\n",
    "        print '%s doesnot contain null values. PROCEED' %(col_name)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_bom.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setting up GFS\n",
    "df_gfs['Unnamed: 0'] = pd.to_datetime(df_gfs['Unnamed: 0'])\n",
    "df_gfs.set_index('Unnamed: 0', inplace = True)\n",
    "\n",
    "# Setting up BOM\n",
    "df_bom['Unnamed: 0'] = pd.to_datetime(df_bom['Unnamed: 0'])\n",
    "df_bom.set_index('Unnamed: 0', inplace=True)\n",
    "\n",
    "# Remove duplicated indices in bom\n",
    "df_bom = df_bom.groupby(df_bom.index).first() # Remove the duplicated index\n",
    "\n",
    "\n",
    "# Setting df_gt\n",
    "# df_gt['Unnamed: 0'] = pd.to_datetime(df_gt['Unnamed: 0'])\n",
    "# df_gt.set_index('Unnamed: 0', inplace=True)\n",
    "\n",
    "if df_gfs.index[0]>df_bom.index[0]:\n",
    "    start_date = df_gfs.index[0]\n",
    "else:\n",
    "    start_date = df_bom.index[0]\n",
    "\n",
    "if df_gfs.index[len(df_gfs)-1]<df_bom.index[len(df_bom)-1]:\n",
    "    end_date = df_gfs.index[len(df_gfs)-1]\n",
    "else:\n",
    "    end_date = df_bom.index[len(df_bom)-1]\n",
    "    \n",
    "print 'START AND END DATES ARE: ', start_date, end_date\n",
    "# Chop GFS, BOM and GT from start to end date\n",
    "df_bom = df_bom.loc[start_date:end_date]\n",
    "df_gfs = df_gfs.loc[start_date:end_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'length of bom and gfs after chopping (May Differ): %d and %d ' %(len(df_bom), len(df_gfs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge df_gfs and df\n",
    "df_gfs_bom_combined = df_bom.merge(df_gfs, how='inner', left_index=True, right_index=True)\n",
    "print len(df_gfs_bom_combined)\n",
    "df_gfs_bom_combined.dropna(axis=0, how='all', inplace = True) # Drop the rows where all of the elements are nan\n",
    "print len(df_gfs_bom_combined)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_gfs_bom_combined.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setting bom's selected columns to numeric\n",
    "df_gfs_bom_combined['Temp_Average'] = pd.to_numeric(df_gfs_bom_combined['Temp_Average'], errors='coerce')\n",
    "# df_gfs_bom_combined['Station level pressure in hPa'] = pd.to_numeric(df_gfs_bom_combined['Station level pressure in hPa'], errors='coerce')\n",
    "df_gfs_bom_combined['RelHumidity_Average'] = pd.to_numeric(df_gfs_bom_combined['RelHumidity_Average'], errors='coerce')\n",
    "df_gfs_bom_combined['rainfall_startend_time'] = pd.to_numeric(df_gfs_bom_combined['rainfall_startend_time'], errors='coerce')\n",
    "\n",
    "col_list_bom = ['Temp_Average','RelHumidity_Average']\n",
    "\n",
    "for _col in col_list_bom:\n",
    "    df_gfs_bom_combined = check_and_fill_null_entries_df_col(df_gfs_bom_combined,_col)\n",
    "\n",
    "del col_list_bom\n",
    "\n",
    "# Check Rainfall column\n",
    "df_gfs_bom_combined = check_and_fill_null_entries_df_col_by_value(df_gfs_bom_combined, 'rainfall_startend_time', 0.0)\n",
    "\n",
    "# Prepare GT dataframe\n",
    "df_gt = pd.DataFrame(index=df_gfs_bom_combined.index)\n",
    "df_gt['rainfall_startend_time'] = df_gfs_bom_combined['rainfall_startend_time'].copy()\n",
    "# Drop the groundtruth from dataframe\n",
    "df_gfs_bom_combined.drop('rainfall_startend_time', axis=1, inplace=True)\n",
    "\n",
    "print 'len of df_gt and df_gfs_bom_combined (MUST Match): %d and %d ' %(len(df_gt), len(df_gfs_bom_combined)) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scale GFS cols first\n",
    "df_gfs_bom_combined['pred_temp'] = df_gfs_bom_combined['pred_temp'] - 273.15 # Kelvin to Celsius (BOM unit)\n",
    "df_gfs_bom_combined['pred_surface_pressure'] = df_gfs_bom_combined['pred_surface_pressure']/100.0 # Pa to Hpa (BOM Unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.scatter( df_gfs_bom_combined['Temp_Average'].values ,df_gfs_bom_combined.pred_temp.values)\n",
    "# plt.scatter( df_gfs_bom_combined['RelHumidity_Average'].values ,df_gfs_bom_combined.pred_rel_humidity.values)\n",
    "\n",
    "# df_gfs_bom_combined.Temp_Average.plot()\n",
    "# df_gfs_bom_combined.pred_temp.plot()\n",
    "\n",
    "# df_gfs_bom_combined.RelHumidity_Average.plot(figsize=(15,5))\n",
    "# df_gfs_bom_combined.pred_rel_humidity.plot(figsize=(15,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Select those entries that are greater than 10 mm\n",
    "# outlier_rain_list = df_gt[df_gt.rainfall_t > 10].index.tolist()\n",
    "# # Remove such entries from the dataset\n",
    "# print outlier_rain_list\n",
    "# df_bom.drop(outlier_rain_list,inplace = True)\n",
    "# df_gfs.drop(outlier_rain_list,inplace = True)\n",
    "# df_gt.drop(outlier_rain_list,inplace = True)\n",
    "# print 'outlier entries removed from the dataset\\n'\n",
    "# # Check if GT dataframe is null\n",
    "# df_gt = check_and_fill_null_entries_df(df_gt)\n",
    "# print len(df_bom)\n",
    "# print len(df_gfs)\n",
    "# print len(df_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pressure is missing in BOM file, add it\n",
    "# df_bom.Pressure = df_gfs.pred_surface_pressure.values.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_bom.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_gfs.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Add vars from the past\n",
    "# past_hour = 1\n",
    "\n",
    "# #BOM\n",
    "# df_bom['temp_p1'] = df_bom['Temperature'].shift(past_hour)\n",
    "# df_bom['press_p1'] = df_bom['Pressure'].shift(past_hour)\n",
    "# df_bom['hum_p1'] = df_bom['RelativeHumidity'].shift(past_hour)\n",
    "\n",
    "# #GFS\n",
    "# df_gfs['temp_p1'] = df_gfs['pred_temp'].shift(past_hour)\n",
    "# df_gfs['press_p1'] = df_gfs['pred_surface_pressure'].shift(past_hour)\n",
    "# df_gfs['hum_p1'] = df_gfs['pred_rel_humidity'].shift(past_hour)\n",
    "\n",
    "# # Fill nans backwards\n",
    "# df_bom = check_and_fill_null_entries_df_col_backwards(df_bom, 'temp_p1')\n",
    "# df_bom = check_and_fill_null_entries_df_col_backwards(df_bom, 'press_p1')\n",
    "# df_bom = check_and_fill_null_entries_df_col_backwards(df_bom, 'hum_p1')\n",
    "# df_gfs = check_and_fill_null_entries_df_col_backwards(df_gfs, 'temp_p1')\n",
    "# df_gfs = check_and_fill_null_entries_df_col_backwards(df_gfs, 'press_p1')\n",
    "# df_gfs = check_and_fill_null_entries_df_col_backwards(df_gfs, 'hum_p1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_gfs_bom_combined.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_gfs_bom_combined.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_list = ['Temp_Average', 'RelHumidity_Average',\n",
    "           'pred_cloud_cover', 'pred_cloud_cover_bound_cloud_layer', 'pred_convective_cloud',\n",
    "           'pred_dewp', 'pred_high_tcc', 'pred_low_tcc', 'pred_lw_rad', 'pred_max_wind_press', 'pred_merid_wind',\n",
    "           'pred_middle_tcc', 'pred_rain_rate', 'pred_rel_humidity', 'pred_sunshine', 'pred_surf_geowind',\n",
    "           'pred_surface_pressure', 'pred_sw_rad', 'pred_temp', 'pred_total_rain', 'pred_ustorm', \n",
    "            'pred_vstorm', 'pred_wind_speed_surf', 'pred_zonal_wind']\n",
    "\n",
    "for _col in col_list:\n",
    "    df_gfs_bom_combined = check_and_fill_null_entries_df_col(df_gfs_bom_combined,_col)\n",
    "\n",
    "df_X = df_gfs_bom_combined[col_list].copy()\n",
    "df_Y = df_gt.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make sure that df_X is not null\n",
    "df_X_bool = df_X.isnull().any().any()\n",
    "print df_X_bool, 'THIS MUST BE FALSE' # This must be FALSE\n",
    "\n",
    "# df_X.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare test observations from GFS\n",
    "# col_list_gfs = ['pred_temp', 'pred_surface_pressure', 'pred_rel_humidity',\n",
    "#            'temp_p1', 'press_p1', 'hum_p1']\n",
    "# df_X_test = df_gfs[col_list_gfs].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make sure that df_X is not null\n",
    "# df_X_bool = df_X_test.isnull().any().any()\n",
    "# print df_X_bool, 'THIS MUST BE FALSE' # This must be FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# See if the pressure from BOM and GFS have linear relation among them\n",
    "# plt.scatter(df_X.Pressure.values,df_X.pred_surface_pressure.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.scatter(df_X['Air Temperature in degrees C'].values,df_X.pred_temp.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_X.Pressure.mean()\n",
    "# print df_X['Mean sea level pressure in hPa'].min(), df_X['Mean sea level pressure in hPa'].max() \n",
    "# print df_X['Station level pressure in hPa'].min(), df_X['Station level pressure in hPa'].max() \n",
    "print 'min_pressure_gfs: %f max_pressure_gfs=%f\\n'%(df_X.pred_surface_pressure.min(), df_X.pred_surface_pressure.max()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_X['pred_surface_pressure'].loc[df_X['pred_surface_pressure'] > 1018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If pressure is not having a linear relation with GFS predicted pressure, then replace unaccepted values by mean\n",
    "# df_X['Station level pressure in hPa'].loc[df_X['Station level pressure in hPa'] < 900] = df_X.Pressure.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "#              IF SCALING IS REQUIRED \n",
    "# ------------------------------------------------\n",
    "# Prepare test and train points for scaling\n",
    "tot_points = len(df_X)\n",
    "train_points = int(np.floor(train_ratio*tot_points)) \n",
    "print 'tot_points: ', tot_points, ' train_points: ', train_points\n",
    "\n",
    "df_data_matrix = df_X.as_matrix()\n",
    "print 'Shape of data matrix: ',df_data_matrix.shape\n",
    "\n",
    "# Generate Train Sequence\n",
    "x_train = np.copy(df_data_matrix[0:train_points + 1,:])\n",
    "\n",
    "# Add polynomial features\n",
    "# poly = PolynomialFeatures(degree=2, interaction_only=False)\n",
    "# x_train = poly.fit_transform(x_train)\n",
    "\n",
    "# Normalize data matrix with zero mean and unit covariance\n",
    "# http://scikit-learn.org/stable/modules/preprocessing.html\n",
    "scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "\n",
    "\n",
    "\n",
    "del tot_points\n",
    "del train_points\n",
    "del df_data_matrix\n",
    "del x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data_matrix = df_X.as_matrix()\n",
    "df_gt_rainfall_forecast_matrix = df_Y.as_matrix()\n",
    "\n",
    "# Adding Polynomial features\n",
    "# Apply poly transform on df_data_matrix\n",
    "# df_data_matrix = poly.fit_transform(df_data_matrix)\n",
    "\n",
    "print 'Shape of data matrix: ',df_data_matrix.shape\n",
    "print 'Shape of label matrix: ',df_gt_rainfall_forecast_matrix.shape\n",
    "\n",
    "# Normalize data matrix with zero mean and unit covariance\n",
    "# http://scikit-learn.org/stable/modules/preprocessing.html\n",
    "# df_data_matrix = preprocessing.scale(df_data_matrix)\n",
    "df_data_matrix = scaler.transform(df_data_matrix)\n",
    "\n",
    "\n",
    "data_matrix = np.copy(df_data_matrix)\n",
    "label_matrix = np.copy(df_gt_rainfall_forecast_matrix)\n",
    "\n",
    "print \"InputFeedForwardNetwork: \",data_matrix.shape\n",
    "print \"OutputFeedForwardNetwork: \",label_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare test and train points\n",
    "tot_points = data_matrix.shape[0]\n",
    "train_points = int(np.floor(train_ratio*tot_points)) \n",
    "test_points = tot_points - train_points\n",
    "print 'tot_points: ', tot_points, ' train_points: ', train_points, ' test_points: ', test_points\n",
    "\n",
    "# Generate Train Sequence\n",
    "x_train = np.copy(data_matrix[0:train_points + 1,:])\n",
    "y_train = np.copy(label_matrix[0:train_points + 1,:])\n",
    "# Generate Test Sequence\n",
    "x_test = np.copy(data_matrix[train_points + 1:tot_points + 1 ,:])\n",
    "y_test = np.copy(label_matrix[train_points + 1:tot_points + 1 ,:])\n",
    "\n",
    "print 'x_train_shape: ', x_train.shape, ' y_train_shape: ', y_train.shape\n",
    "print 'x_test_shape: ', x_test.shape, ' y_test_shape: ', y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'train rainfall in mm: %f' %(np.sum(y_train))\n",
    "print 'test rainfall in mm: %f' %(np.sum(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.plot(df_gt.rainfall_since_last_hour.index, df_gt.rainfall_since_last_hour.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Test matrix\n",
    "# df_test_matrix = df_X_test.as_matrix()\n",
    "# # Apply poly transform on df_test_matrix\n",
    "# df_test_matrix = poly.fit_transform(df_test_matrix)\n",
    "\n",
    "# # Scale it \n",
    "# df_test_matrix = scaler.transform(df_test_matrix)\n",
    "\n",
    "# # Create new variable for test matrix\n",
    "# testdata_matrix = np.copy(df_test_matrix)\n",
    "\n",
    "# # Print its shape\n",
    "# print ' (Must equalize data matrix) Shape of test data matrix: ',testdata_matrix.shape\n",
    "\n",
    "# # Prepare test and train points\n",
    "# tot_points = data_matrix.shape[0]\n",
    "# train_points = data_matrix.shape[0]\n",
    "# test_points = testdata_matrix.shape[0]\n",
    "# print 'tot_points: ', tot_points, ' train_points: ', train_points, ' test_points: ', test_points\n",
    "\n",
    "# # Generate Train Sequence\n",
    "# x_train = np.copy(data_matrix)\n",
    "# y_train = np.copy(label_matrix)\n",
    "# # Generate Test Sequence\n",
    "# x_test = np.copy(testdata_matrix)\n",
    "# y_test = np.copy(label_matrix)\n",
    "\n",
    "# print 'x_train_shape: ', x_train.shape, ' y_train_shape: ', y_train.shape\n",
    "# print 'x_test_shape: ', x_test.shape, ' y_test_shape: ', y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=x_train.shape[1], init='normal', activation='relu')) #1st Hidden Layer\n",
    "model.add(Dense(200, init='normal', activation='relu')) #2nd Hidden Layer\n",
    "model.add(Dense(300, init='normal', activation='relu')) #2nd Hidden Layer\n",
    "model.add(Dense(y_train.shape[1], init='normal')) #output layer\n",
    "    \n",
    "# compile model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filepath_savemodel = 'C:/Shaukat/code/rainfall/model_stored/feedforward/sensordata/houston/weights.{epoch:02d}-{val_loss:.2f}.h5'\n",
    "filepath_savemodel = 'C:/Shaukat/code/rainfall/model_stored/feedforward/sensordata/houston/model_v1_best.h5'\n",
    "# filepath_csvlogger = 'C:/Shaukat/code/rainfall/model_stored/feedforward/sensordata/houston/training_s2.log'\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath_savemodel, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "# csv_logger = CSVLogger(filepath_csvlogger, separator=',', append=False)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0.001)\n",
    "\n",
    "# history = model.fit(x_train, y_train, nb_epoch=1000, verbose=1, validation_data=(x_test, y_test), batch_size = 32, shuffle=True, callbacks=[checkpointer, csv_logger, reduce_lr])\n",
    "history = model.fit(x_train, y_train, nb_epoch=100, verbose=1, validation_data=(x_test, y_test), batch_size = 32, shuffle=True, callbacks=[checkpointer, reduce_lr])\n",
    "# history = model.fit(x_train, y_train, nb_epoch=100, verbose=1, validation_data=(x_test, y_test), batch_size = train_points, shuffle=True, callbacks=[checkpointer])\n",
    "# history = model.fit(x_train, y_train, nb_epoch=500, verbose=1, validation_data=(x_test, y_test), batch_size = 32, callbacks=[checkpointer])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('C:/Shaukat/code/rainfall/model_stored/feedforward/sensordata/houston/model_v1_last.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vec_test = np.copy(y_test)\n",
    "print vec_test.shape\n",
    "print 'total test points: ', vec_test.shape[0]\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "pred_vec = np.copy(predictions)\n",
    "print pred_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess block\n",
    "threshold = 0.01\n",
    "preproc_y = deepcopy(pred_vec)\n",
    "preproc_y[preproc_y<=threshold] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For plot\n",
    "time_index = np.arange(0,vec_test.shape[0],1)\n",
    "time_index = time_index.reshape(((len(time_index),1)))\n",
    "\n",
    "plt_lower_limit = 0\n",
    "plt_upper_limit = 1000\n",
    "\n",
    "plt.figure(figsize=(25,8))\n",
    "\n",
    "# plot\n",
    "\n",
    "# Full plot\n",
    "# plt.plot(time_index,vec_test,label=\"GroundTruth\",color='b')\n",
    "# plt.plot(time_index,pred_vec,label=\"Prediction\",color='g')\n",
    "\n",
    "# Unpreprocessed Plot\n",
    "plt.plot(time_index[plt_lower_limit:plt_upper_limit],vec_test[plt_lower_limit:plt_upper_limit],label=\"GroundTruth\",color='b')\n",
    "plt.plot(time_index[plt_lower_limit:plt_upper_limit],pred_vec[plt_lower_limit:plt_upper_limit],label=\"Prediction\",color='g')\n",
    "\n",
    "# Preprocessed Plot\n",
    "# plt.plot(time_index[plt_lower_limit:plt_upper_limit],vec_test[plt_lower_limit:plt_upper_limit],label=\"GroundTruth\",color='b')\n",
    "# plt.plot(time_index[plt_lower_limit:plt_upper_limit],preproc_y[plt_lower_limit:plt_upper_limit],label=\"Prediction\",color='g')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Rainfall prediction in mm')\n",
    "plt.legend()\n",
    "plt.title('Groundtruth and Predicted Rainfall per hour')\n",
    "# img_filename = 'C:\\\\Users\\\\ShaukatAbidi\\\\Documents\\\\shaukat_python_progs\\\\learning\\\\time_series\\\\feed_forward_nn\\\\models\\\\s7_images\\\\'+str(test_ex)+'.png'\n",
    "# plt.savefig(img_filename, bbox_inches='tight')\n",
    "# plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print y_test.shape, predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving an array to file\n",
    "#np.savetxt('predictions.out', predictions, delimiter=',')   # predictions is an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluate training \n",
    "model.evaluate(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vec_train = np.copy(y_train)\n",
    "print vec_train.shape\n",
    "print 'total train points: ', vec_train.shape[0]\n",
    "\n",
    "pred_on_gt = model.predict(x_train)\n",
    "pred_on_gt_vec = np.copy(pred_on_gt)\n",
    "print pred_on_gt_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For plot\n",
    "time_index = np.arange(0,vec_train.shape[0],1)\n",
    "time_index = time_index.reshape(((len(time_index),1)))\n",
    "\n",
    "plt_lower_limit = 0\n",
    "plt_upper_limit = 500\n",
    "\n",
    "plt.figure(figsize=(25,8))\n",
    "\n",
    "# plot\n",
    "\n",
    "# Full plot\n",
    "# plt.plot(time_index,vec_train,label=\"GroundTruth\",color='b')\n",
    "# plt.plot(time_index,pred_on_gt_vec,label=\"Prediction\",color='g')\n",
    "\n",
    "# Unpreprocessed Plot\n",
    "plt.plot(time_index[plt_lower_limit:plt_upper_limit],vec_train[plt_lower_limit:plt_upper_limit],label=\"GroundTruth\",color='b')\n",
    "plt.plot(time_index[plt_lower_limit:plt_upper_limit],pred_on_gt_vec[plt_lower_limit:plt_upper_limit],label=\"Prediction\",color='g')\n",
    "\n",
    "# Preprocessed Plot\n",
    "# plt.plot(time_index[plt_lower_limit:plt_upper_limit],vec_test[plt_lower_limit:plt_upper_limit],label=\"GroundTruth\",color='b')\n",
    "# plt.plot(time_index[plt_lower_limit:plt_upper_limit],preproc_y[plt_lower_limit:plt_upper_limit],label=\"Prediction\",color='g')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Rainfall prediction in mm')\n",
    "plt.legend()\n",
    "plt.title('Groundtruth and Predicted Rainfall per hour')\n",
    "# img_filename = 'C:\\\\Users\\\\ShaukatAbidi\\\\Documents\\\\shaukat_python_progs\\\\learning\\\\time_series\\\\feed_forward_nn\\\\models\\\\s7_images\\\\'+str(test_ex)+'.png'\n",
    "# plt.savefig(img_filename, bbox_inches='tight')\n",
    "# plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:sequential_regression]",
   "language": "python",
   "name": "conda-env-sequential_regression-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
