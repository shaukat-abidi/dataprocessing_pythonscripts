{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run it once and Comment\n",
    "# import sys\n",
    "# sys.path.append('C:\\\\Shaukat\\\\code\\\\functions_implemented')\n",
    "# print sys.path\n",
    "# import functions_implemented as fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import scipy.stats as stats\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from copy import deepcopy\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All parameter gradients will be clipped to\n",
    "# a maximum norm of 1.\n",
    "# sgd = optimizers.rmsprop(lr=0.01, clipnorm=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_gfs = pd.read_csv(\"C:/Shaukat/code/data_rep/gfs/csv_files/gfs_lat_-43.0_lon_147.25.csv\")\n",
    "df_bom = pd.read_csv(\"C:/Shaukat/code/data_rep/bom/historical_data/2013-2016/TAS/csv/HM01X_Data_094029_999999999405613_gfs_lat_-43.0_lon_147.25.csv\")\n",
    "df_gfs.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df_bom.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_bom.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setting up GFS\n",
    "df_gfs['utc_datetime'] = pd.to_datetime(df_gfs['utc_datetime'])\n",
    "df_gfs.set_index('utc_datetime', inplace = True)\n",
    "\n",
    "# Setting up BOM\n",
    "# df_bom['date_only'] = df_bom.UtcTime.dt.date\n",
    "df_bom.utc_mmddyyyy = pd.to_datetime(df_bom.utc_mmddyyyy)\n",
    "df_bom.set_index('utc_mmddyyyy', inplace=True)\n",
    "\n",
    "# Trim df_bom and df_gfs from 15 jan 2015\n",
    "df_bom = df_bom.loc['2015-01-15 00:00:00':]\n",
    "df_gfs = df_gfs.loc['2015-01-15 00:00:00':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_gfs.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_bom.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare groundtruth dataframe from BOM \n",
    "tot_gfs_entries = len(df_gfs.index)\n",
    "df_gt = pd.DataFrame(index=df_gfs.index, columns=['rainfall_t+1','rainfall_t+2','rainfall_t+3'])\n",
    "for dt_gfs in range(0,tot_gfs_entries):\n",
    "    # Iterate each date in GFS and get the groundtruth rainfall from BOM\n",
    "    \n",
    "    date_a = df_gfs.index[dt_gfs] # Forecast time in gfs (made every three hours)\n",
    "    date_t1 = date_a + timedelta(hours=1) # rainfall_t+1\n",
    "    date_t2 = date_a + timedelta(hours=2) # rainfall_t+2\n",
    "    date_t3 = date_a + timedelta(hours=3) # rainfall_t+3\n",
    "    #print 'processing: ', date_a \n",
    "\n",
    "    # df_patch for t+1 (Gather groundtruth rainfall from BOM)\n",
    "    df_patch = df_bom.loc[date_a:date_t1].copy()\n",
    "    rainfall_t1 = df_patch.rainfall_since_last_hour.sum()\n",
    "    #print 'date_t1: ', date_t1 \n",
    "\n",
    "    \n",
    "    # df_patch for t+2 (Gather groundtruth rainfall from BOM)\n",
    "    df_patch = df_bom.loc[date_t1:date_t2].copy()\n",
    "    rainfall_t2 = df_patch.rainfall_since_last_hour.sum()\n",
    "    #print 'date_t2: ', date_t2 \n",
    "\n",
    "    \n",
    "    # df_patch for t+3 (Gather groundtruth rainfall from BOM)\n",
    "    df_patch = df_bom.loc[date_t2:date_t3].copy()\n",
    "    rainfall_t3 = df_patch.rainfall_since_last_hour.sum()\n",
    "    #print 'date_t3: ', date_t3, '\\n' \n",
    "\n",
    "    \n",
    "    # Add it to the groundtruth df\n",
    "    df_gt.loc[date_a] = [rainfall_t1, rainfall_t2, rainfall_t3]\n",
    "    \n",
    "    # Delete variables\n",
    "    del date_a\n",
    "    del date_t1\n",
    "    del date_t2\n",
    "    del date_t3\n",
    "    del rainfall_t1\n",
    "    del rainfall_t2\n",
    "    del rainfall_t3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rainfall_t+1</th>\n",
       "      <th>rainfall_t+2</th>\n",
       "      <th>rainfall_t+3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>utc_datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-04-03 18:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-04 00:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-04 06:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-04 12:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-04 18:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-05 00:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-05 06:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-05 12:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-05 18:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-06 00:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    rainfall_t+1 rainfall_t+2 rainfall_t+3\n",
       "utc_datetime                                              \n",
       "2015-04-03 18:00:00            0            0            0\n",
       "2015-04-04 00:00:00            0            0            0\n",
       "2015-04-04 06:00:00            0            0            0\n",
       "2015-04-04 12:00:00            0            0            0\n",
       "2015-04-04 18:00:00          NaN          NaN          NaN\n",
       "2015-04-05 00:00:00            0          0.2          0.2\n",
       "2015-04-05 06:00:00            0            0            0\n",
       "2015-04-05 12:00:00            0            0            0\n",
       "2015-04-05 18:00:00            0            0            0\n",
       "2015-04-06 00:00:00            0            0            0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gt.ix[315:325]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if df_gt is having null values then fill NaNs\n",
    "# df_gt.fillna(method='ffill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_gt contains some null values. Filling these null values\n",
      "df_gt doesnot contain null values. PROCEED\n"
     ]
    }
   ],
   "source": [
    "# Check if dataframe is null\n",
    "is_df_gt_nan = df_gt.isnull().any().any()\n",
    "if (is_df_gt_nan):\n",
    "    print 'df_gt contains some null values. Filling these null values'\n",
    "    df_gt.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Check again if dataframe is null\n",
    "is_df_gt_nan = df_gt.isnull().any().any()\n",
    "if not is_df_gt_nan:\n",
    "    print 'df_gt doesnot contain null values. PROCEED'\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rainfall_t+1</th>\n",
       "      <th>rainfall_t+2</th>\n",
       "      <th>rainfall_t+3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>utc_datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-04-03 18:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-04 00:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-04 06:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-04 12:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-04 18:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-05 00:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-05 06:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-05 12:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-05 18:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-06 00:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     rainfall_t+1  rainfall_t+2  rainfall_t+3\n",
       "utc_datetime                                                 \n",
       "2015-04-03 18:00:00           0.0           0.0           0.0\n",
       "2015-04-04 00:00:00           0.0           0.0           0.0\n",
       "2015-04-04 06:00:00           0.0           0.0           0.0\n",
       "2015-04-04 12:00:00           0.0           0.0           0.0\n",
       "2015-04-04 18:00:00           0.0           0.0           0.0\n",
       "2015-04-05 00:00:00           0.0           0.2           0.2\n",
       "2015-04-05 06:00:00           0.0           0.0           0.0\n",
       "2015-04-05 12:00:00           0.0           0.0           0.0\n",
       "2015-04-05 18:00:00           0.0           0.0           0.0\n",
       "2015-04-06 00:00:00           0.0           0.0           0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gt.ix[315:325]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_gt.to_csv(\"check_hob_gt_rainfall.csv\")\n",
    "# X: df_gfs\n",
    "# Y: df_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'apparent_temperature', u'cloud_water', u'dew_point_temp',\n",
      "       u'encoded_time', u'frozen_precipt', u'haines_index', u'lat', u'lon',\n",
      "       u'potential_temp', u'precipitable_water', u'press_l1', u'press_l103',\n",
      "       u'press_l6', u'press_l7', u'rel_humidity_level_a',\n",
      "       u'rel_humidity_level_b', u'rel_humidity_level_c',\n",
      "       u'rel_humidity_level_d', u'rel_humidity_level_e',\n",
      "       u'rel_humidity_level_f', u'sunshine_duration', u'surface_lifted_ind',\n",
      "       u'temp_level_a', u'temp_level_b', u'temp_level_c', u'temp_level_d',\n",
      "       u'temp_level_e', u'u_comp_storm', u'u_wind_level_a', u'u_wind_level_b',\n",
      "       u'u_wind_level_c', u'u_wind_level_d', u'u_wind_level_e',\n",
      "       u'v_comp_storm', u'v_comp_wind_3', u'v_comp_wind_a', u'v_comp_wind_b',\n",
      "       u'v_comp_wind_c', u'v_comp_wind_d', u'vert_vel', u'wilting_point',\n",
      "       u'wind_speed_gust'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print df_gfs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_list = ['apparent_temperature', 'cloud_water', 'dew_point_temp',\n",
    "            'potential_temp', 'precipitable_water', 'press_l1', 'press_l103',\n",
    "       'press_l6', 'press_l7', 'rel_humidity_level_a',\n",
    "       'rel_humidity_level_b', 'rel_humidity_level_c',\n",
    "       'rel_humidity_level_d', 'rel_humidity_level_e',\n",
    "       'rel_humidity_level_f', 'sunshine_duration', 'surface_lifted_ind',\n",
    "       'temp_level_a', 'temp_level_b', 'temp_level_c', 'temp_level_d',\n",
    "       'temp_level_e', 'u_comp_storm', 'u_wind_level_a', 'u_wind_level_b',\n",
    "       'u_wind_level_c', 'u_wind_level_d', 'u_wind_level_e',\n",
    "       'v_comp_storm', 'v_comp_wind_3', 'v_comp_wind_a', 'v_comp_wind_b',\n",
    "       'v_comp_wind_c', 'v_comp_wind_d', 'vert_vel', 'wind_speed_gust']\n",
    "\n",
    "# col_list = ['apparent_temperature', 'cloud_water']\n",
    "\n",
    "df_X = df_gfs[col_list].copy()\n",
    "df_Y = df_gt.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False THIS MUST BE FALSE\n"
     ]
    }
   ],
   "source": [
    "# Make sure that df_X is not null\n",
    "df_X_bool = df_X.isnull().any().any()\n",
    "print df_X_bool, 'THIS MUST BE FALSE' # This must be FALSE\n",
    "\n",
    "# df_X.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_X.to_csv('input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data matrix:  (3198L, 37L)\n",
      "Shape of label matrix:  (3198L, 3L)\n",
      "Total Sample:  3198\n",
      "Dimension of each sample:  37\n",
      "LSTM input:  (3195L, 3L, 37L)\n",
      "LSTM output:  (3195L, 3L)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ShaukatAbidi\\Anaconda3\\envs\\sequential_regression\\lib\\site-packages\\sklearn\\preprocessing\\data.py:177: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "df_data_matrix = df_X.as_matrix()\n",
    "df_gt_rainfall_forecast_matrix = df_Y.as_matrix()\n",
    "print 'Shape of data matrix: ',df_data_matrix.shape\n",
    "print 'Shape of label matrix: ',df_gt_rainfall_forecast_matrix.shape\n",
    "\n",
    "# Normalize data matrix with zero mean and unit covariance\n",
    "# http://scikit-learn.org/stable/modules/preprocessing.html\n",
    "df_data_matrix = preprocessing.scale(df_data_matrix)\n",
    "\n",
    "data_matrix = np.copy(df_data_matrix)\n",
    "label_matrix = np.copy(df_gt_rainfall_forecast_matrix)\n",
    "\n",
    "sliding_window = 3\n",
    "tot_samples = data_matrix.shape[0]\n",
    "sample_dimension = data_matrix.shape[1]\n",
    "print \"Total Sample: \",tot_samples\n",
    "print \"Dimension of each sample: \",sample_dimension\n",
    "    \n",
    "# Accumulate examples here in this list\n",
    "input_lstm = []\n",
    "label_lstm = []\n",
    "label_entry = sliding_window\n",
    "\n",
    "    \n",
    "for sequence in range(0,tot_samples-sliding_window):\n",
    "    input_lstm.append(data_matrix[sequence:sequence+sliding_window, :])\n",
    "    label_lstm.append(label_matrix[label_entry,0:3])\n",
    "    #print \"sequence, sequence+sliding_window, label_entry: \",sequence,sequence+sliding_window,label_entry\n",
    "    label_entry = label_entry + 1\n",
    "\n",
    "\n",
    "return_data_matrix_lstm = np.array(input_lstm)\n",
    "return_label_matrix_lstm = np.array(label_lstm)\n",
    "\n",
    "print \"LSTM input: \",return_data_matrix_lstm.shape\n",
    "print \"LSTM output: \",return_label_matrix_lstm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare test and train points\n",
    "tot_points = return_data_matrix_lstm.shape[0]\n",
    "train_points = int(np.floor(0.61*tot_points)) \n",
    "test_points = tot_points - train_points\n",
    "print 'tot_points: ', tot_points, ' train_points: ', train_points, ' test_points: ', test_points\n",
    "\n",
    "# Generate Train Sequence\n",
    "x_train = np.copy(return_data_matrix_lstm[0:train_points + 1,:])\n",
    "y_train = np.copy(return_label_matrix_lstm[0:train_points + 1,:])\n",
    "# Generate Test Sequence\n",
    "x_test = np.copy(return_data_matrix_lstm[train_points + 1:tot_points + 1 ,:])\n",
    "y_test = np.copy(return_label_matrix_lstm[train_points + 1:tot_points + 1 ,:])\n",
    "\n",
    "print 'x_train_shape: ', x_train.shape, ' y_train_shape: ', y_train.shape\n",
    "print 'x_test_shape: ', x_test.shape, ' y_test_shape: ', y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_dim = x_train.shape[2] # Input vector dimension for LSTM (N x timesteps x dim of seq)\n",
    "output_dim = y_train.shape[1] # Output dimension for LSTM (temperature for t+1,t+2,t+3)\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "layers = [input_dim, 50, 100, output_dim]\n",
    "\n",
    "model.add(LSTM(input_dim=layers[0],output_dim=layers[1],return_sequences=True))\n",
    "#model.add(Dropout(0.1))\n",
    "\n",
    "model.add(LSTM(layers[2], return_sequences=False))\n",
    "#model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(output_dim=layers[3]))\n",
    "model.add(Activation(\"linear\"))\n",
    "    \n",
    "# compile model\n",
    "# start = time.time()\n",
    "# model.compile(loss=\"mse\", optimizer=\"rmsprop\")\n",
    "# print \"Compilation Time : \", time.time() - start\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "# model.compile(loss='mean_squared_error', optimizer=sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filepath_savemodel = 'C:\\\\Shaukat\\\\code\\\\rainfall\\\\model_stored\\\\weights.{epoch:02d}-{val_loss:.2f}.h5'\n",
    "filepath_savemodel = 'C:\\\\Shaukat\\\\code\\\\rainfall\\\\model_stored\\\\hobart_airport\\\\model_lstm_v1.h5'\n",
    "# filepath_csvlogger = 'C:\\\\Shaukat\\\\code\\\\rainfall\\\\model_stored\\\\training_s2.log'\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath_savemodel, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "# csv_logger = CSVLogger(filepath_csvlogger, separator=',', append=False)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0.001)\n",
    "\n",
    "# history = model.fit(x_train, y_train, nb_epoch=1000, verbose=1, validation_data=(x_test, y_test), batch_size = 32, shuffle=True, callbacks=[checkpointer, csv_logger, reduce_lr])\n",
    "# history = model.fit(x_train, y_train, nb_epoch=10, verbose=1, validation_data=(x_test, y_test), batch_size = train_points, shuffle=True, callbacks=[checkpointer, reduce_lr])\n",
    "history = model.fit(x_train, y_train, nb_epoch=10, verbose=1, validation_data=(x_test, y_test), batch_size = train_points, shuffle=True, callbacks=[checkpointer, reduce_lr])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vec_test = y_test.flatten()\n",
    "vec_test = vec_test.reshape((len(vec_test),1))\n",
    "print vec_test.shape\n",
    "print 'total test points: ', vec_test.shape[0]\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "pred_vec = predictions.flatten()\n",
    "pred_vec = pred_vec.reshape((len(pred_vec),1))\n",
    "print pred_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preprocess block\n",
    "# threshold = 1.0\n",
    "# preproc_y = deepcopy(pred_vec)\n",
    "# preproc_y[preproc_y<=threshold] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For plot\n",
    "time_index = np.arange(0,vec_test.shape[0],1)\n",
    "time_index = time_index.reshape(((len(time_index),1)))\n",
    "\n",
    "plt_lower_limit = 0\n",
    "plt_upper_limit = 100\n",
    "\n",
    "plt.figure(figsize=(25,8))\n",
    "\n",
    "# plot\n",
    "\n",
    "# Full plot\n",
    "# plt.plot(time_index,vec_test,label=\"GroundTruth\",color='b')\n",
    "# plt.plot(time_index,pred_vec,label=\"Prediction\",color='g')\n",
    "\n",
    "# Unpreprocessed Plot\n",
    "plt.plot(time_index[plt_lower_limit:plt_upper_limit],vec_test[plt_lower_limit:plt_upper_limit],label=\"GroundTruth\",color='b')\n",
    "plt.plot(time_index[plt_lower_limit:plt_upper_limit],pred_vec[plt_lower_limit:plt_upper_limit],label=\"Prediction\",color='g')\n",
    "\n",
    "# Preprocessed Plot\n",
    "# plt.plot(time_index[plt_lower_limit:plt_upper_limit],vec_test[plt_lower_limit:plt_upper_limit],label=\"GroundTruth\",color='b')\n",
    "# plt.plot(time_index[plt_lower_limit:plt_upper_limit],preproc_y[plt_lower_limit:plt_upper_limit],label=\"Prediction\",color='g')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Rainfall prediction in mm')\n",
    "plt.legend()\n",
    "plt.title('Groundtruth and Predicted Rainfall per hour')\n",
    "# img_filename = 'C:\\\\Users\\\\ShaukatAbidi\\\\Documents\\\\shaukat_python_progs\\\\learning\\\\time_series\\\\feed_forward_nn\\\\models\\\\s7_images\\\\'+str(test_ex)+'.png'\n",
    "# plt.savefig(img_filename, bbox_inches='tight')\n",
    "# plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:sequential_regression]",
   "language": "python",
   "name": "conda-env-sequential_regression-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
